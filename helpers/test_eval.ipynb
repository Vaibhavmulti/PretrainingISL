{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/el/miniconda3/envs/nmt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# # Set the visible GPU devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "MAX_FRAMES = 300\n",
    "max_length_decoder = 128\n",
    "batch_size = 64\n",
    "num_encoder_layers = 4 #4\n",
    "num_decoder_layers = 4 #4\n",
    "encoder_hidden_size = 512 #512\n",
    "decoder_hidden_size = 512 #512\n",
    "num_attention_heads = 8\n",
    "dropout = 0.1\n",
    "num_keypoints = 152\n",
    "WEIGTH_DECAY = 0.01\n",
    "learning_rate = 3e-4 #3e-4 \n",
    "num_beams = 3\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import gc\n",
    "import collections\n",
    "import math\n",
    "import ast\n",
    "import collections\n",
    "import math\n",
    "import sacrebleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig, BertModel,\n",
    "    GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    get_constant_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from bleu_cal import quick_bleu_metric\n",
    "from dataloaders import FeatureVectorDataset, FeatureVectorDataset_Isign\n",
    "from pose_format import Pose\n",
    "from pose_format.pose_visualizer import PoseVisualizer\n",
    "from itertools import cycle\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed()\n",
    "\n",
    "POSE_DIR_ISIGN = \"/DATA7/vaibhav/isign/Data/iSign-poses_v1.1/\"\n",
    "STEP_FRAMES_ISIGN = None\n",
    "ADD_NOISE_ISIGN = False\n",
    "\n",
    "train_df2 = pd.read_csv(\"/DATA3/vaibhav/isign/PretrainingISL/isign_new.csv\")\n",
    "#train_df = pd.read_csv('/DATA3/vaibhav/isign/PretrainingISL/train_MT16M.csv')\n",
    "train_df = pd.read_csv('/DATA3/vaibhav/isign/PretrainingISL/train_BLIMPCISLR.csv')\n",
    "all_sequences_target = train_df['text'].tolist() + train_df2['text'].tolist()\n",
    "\n",
    "# Initialize and train the tokenizer\n",
    "tokenizer_model = models.BPE()\n",
    "tokenizer = Tokenizer(tokenizer_model)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "vocab_size_decoder = 15000\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size_decoder,\n",
    "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\", \"<PERSON>\", \"<UNKNOWN>\"]\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(all_sequences_target, trainer=trainer)\n",
    "# Save the tokenizer\n",
    "#Make tokenizer_file if it does not exist\n",
    "\n",
    "if not os.path.exists('tokenizer_file'):\n",
    "    os.makedirs('tokenizer_file')\n",
    "\n",
    "tokenizer.save(\"tokenizer_file/target_tokenizer.json\")\n",
    "\n",
    "#Load the tokenizer as a PreTrainedTokenizerFast\n",
    "tokenizer_target = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer_file/target_tokenizer.json\")\n",
    "tokenizer_target.add_special_tokens({\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    'additional_special_tokens': ['<PERSON>', '<UNKNOWN>']\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint_path_isignB4 = '/DATA3/vaibhav/isign/PretrainingISL/predictions_new/BLIMP_Pretraining_BlimpFrameMatchA100_Linear60kBPE0.85Threshold_PT1_best_model_checkpoint_isignB4.pth'\n",
    "                                \n",
    "save_name = \"IsignMLIMP_PT1\"\n",
    "\n",
    "eval_df2 = pd.read_csv('/DATA7/vaibhav/tokenization/val_split_unicode_filtered.csv')\n",
    "#eval_df2 = pd.read_csv('/DATA7/vaibhav/tokenization/test_split_unicode_filtered.csv')\n",
    "\n",
    "eval2_video_uids = eval_df2['uid'].tolist()\n",
    "eval2_labels = [f'<s>{text}</s>' for text in eval_df2['text'].tolist()]\n",
    "eval2_labels = tokenizer_target(eval2_labels, max_length=max_length_decoder, padding=\"max_length\", truncation=True)['input_ids']\n",
    "eval2_dataset = FeatureVectorDataset_Isign(eval2_video_uids, tokenizer_target, \n",
    "                                        MAX_FRAMES, POSE_DIR_ISIGN, eval2_labels, \n",
    "                                        step_frames=STEP_FRAMES_ISIGN, add_noise = ADD_NOISE_ISIGN)\n",
    "eval2_loader = DataLoader(eval2_dataset, batch_size=batch_size, num_workers=2, pin_memory=True, prefetch_factor=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_embd\": 512,\n",
      "  \"n_head\": 8,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 4,\n",
      "  \"n_positions\": 128,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 15000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoder Configuration and Model\n",
    "encoder_config = BertConfig(\n",
    "    hidden_size=encoder_hidden_size,\n",
    "    num_hidden_layers=num_encoder_layers,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    hidden_dropout_prob=dropout,  # Dropout after fully connected layers\n",
    "    attention_probs_dropout_prob=dropout,  # Dropout on attention weights\n",
    ")\n",
    "#encoder = BertForCausalLM(encoder_config)\n",
    "encoder = BertModel(encoder_config)\n",
    "print(encoder_config)\n",
    "\n",
    "# Decoder Configuration and Model\n",
    "decoder_config = GPT2Config(\n",
    "    vocab_size=len(tokenizer_target),\n",
    "    n_positions=max_length_decoder, # We have padded and truncated to 128\n",
    "    n_embd=decoder_hidden_size,\n",
    "    n_layer=num_decoder_layers,\n",
    "    n_head=num_attention_heads,\n",
    "    pad_token_id=tokenizer_target.pad_token_id,\n",
    "    bos_token_id=tokenizer_target.bos_token_id,\n",
    "    eos_token_id=tokenizer_target.eos_token_id,\n",
    "    add_cross_attention=True,  # Important for Seq2Seq models (Can't find this on HF docs)\n",
    "    embd_pdrop=dropout,  # Dropout on embeddings \n",
    "    attn_pdrop=dropout,  # Dropout on attention probabilities \n",
    "    resid_pdrop=dropout  # Dropout on residual connections \n",
    ")\n",
    "print(decoder_config)\n",
    "decoder = GPT2LMHeadModel(decoder_config)\n",
    "\n",
    "########################################################\n",
    "#decoder.resize_token_embeddings(len(tokenizer_target))\n",
    "########################################################\n",
    "\n",
    "# Linear layer to project feature vectors to the expected input shape\n",
    "class FeatureProjection(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims = 1024):\n",
    "        super(FeatureProjection, self).__init__()\n",
    "        # self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dims)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dims, output_dim)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.linear(x)\n",
    "        x = self.gelu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Combine Encoder and Decoder into EncoderDecoderModel\n",
    "feature_projection = FeatureProjection(num_keypoints, encoder_config.hidden_size)\n",
    "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "########################################################################\n",
    "#model.decoder.resize_token_embeddings(len(tokenizer_target))\n",
    "########################################################################\n",
    "\n",
    "# Tie weights (optional)\n",
    "model.config.decoder_start_token_id = tokenizer_target.bos_token_id\n",
    "model.config.eos_token_id = tokenizer_target.eos_token_id\n",
    "model.config.pad_token_id = tokenizer_target.pad_token_id\n",
    "model.config.vocab_size = decoder_config.vocab_size\n",
    "model.config.max_length = max_length_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, feature_projection, optimizer, scheduler, checkpoint_path):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        feature_projection.load_state_dict(checkpoint['feature_projection_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        current_step = checkpoint['current_step']\n",
    "        best_val_B4 = checkpoint['best_val_B4']\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))  # Backwards compatibility\n",
    "        best_val_B4_isign = checkpoint['best_val_B4_isign']\n",
    "        best_val_B1_isign = checkpoint['best_val_B1_isign']\n",
    "        best_val_loss_isign = checkpoint.get('best_val_loss_isign', float('inf'))  # Backwards compatibility\n",
    "        epoch_steps = checkpoint['epoch_steps']\n",
    "        print(f\"Checkpoint loaded, resuming from epoch {start_epoch}\")\n",
    "        print(\"*\"*50)\n",
    "        return start_epoch, best_val_B4, best_val_loss, best_val_B4_isign, best_val_loss_isign, best_val_B1_isign, epoch_steps\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch\")\n",
    "        return 0, 0.0, float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3969568/2868774436.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded, resuming from epoch 1\n",
      "**************************************************\n",
      "Loaded best model checkpoint IsignB4\n",
      "207500\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "feature_projection.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.parameters()) + list(feature_projection.parameters()),\n",
    "    weight_decay=WEIGTH_DECAY,\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# Calculate total steps for scheduler\n",
    "#total_steps = len(train_loader)  \n",
    "# Set warmup to 10% of total steps\n",
    "warmup_steps = 100 #int(warmup_steps_ratio * total_steps)\n",
    "\n",
    "# total_steps = len(train_loader) * num_epochs\n",
    "#warmup_steps = len(train_loader) * warmup_steps_epocs\n",
    "\n",
    "# Create scheduler with linear warmup and constant afterwards\n",
    "\n",
    "scheduler = get_constant_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    #num_training_steps=total_steps  # Will maintain constant lr after warmup\n",
    ")\n",
    "\n",
    "#wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "epoch_steps = 0\n",
    "# Load checkpoint or pretrained weights\n",
    "\n",
    "#/DATA3/vaibhav/isign/PretrainingISL/predictions_new/CISLR_Pretraining_FrameMatch_Linear60kBPE0.85Threshold_PT1_best_model_checkpoint_isignB4.pth                \n",
    "if os.path.exists(best_checkpoint_path_isignB4): #best_checkpoint_path_isignB4\n",
    "    start_epoch, best_val_B4, best_val_loss, best_val_B4_isign, best_val_loss_isign, best_val_B1_isign, epoch_steps = load_checkpoint(\n",
    "        model, feature_projection, optimizer, scheduler, best_checkpoint_path_isignB4\n",
    "    )\n",
    "    start_epoch = 0\n",
    "    print(\"Loaded best model checkpoint IsignB4\")\n",
    "    print(epoch_steps)\n",
    "    print(\"*\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "def model_eval(eval_loader, log_what, best_val_B4,best_val_loss,best_val_B4_isign,\n",
    "               best_val_B1_isign,best_val_loss_isign,counter, current_step,epoch_steps, save_model=False):\n",
    "    model.eval()\n",
    "    feature_projection.eval()\n",
    "    eval_loss = 0.0\n",
    "    all_refs = []\n",
    "    sacre_refs = []\n",
    "    sacre_preds = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_progress = tqdm(eval_loader, desc=f\"Evaluating Epoch {epoch+1}\")\n",
    "        for eval_batch in eval_progress:\n",
    "            input_ids = eval_batch['input_ids'].to(device)\n",
    "            attention_mask = eval_batch['attention_mask'].to(device)\n",
    "            labels = eval_batch['labels'].to(device)\n",
    "            \n",
    "            input_ids = feature_projection(input_ids)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1, encoder_config.hidden_size)\n",
    "\n",
    "            outputs = model(\n",
    "                inputs_embeds=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            eval_loss += outputs.loss.item()\n",
    "            \n",
    "            # Generate predictions with improved parameters\n",
    "            generated_ids = model.generate(\n",
    "                inputs_embeds=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length_decoder,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=0.6,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Process predictions and references\n",
    "            generated_ids = torch.where(\n",
    "                generated_ids == -100,\n",
    "                torch.tensor(tokenizer_target.pad_token_id).to(generated_ids.device),\n",
    "                generated_ids\n",
    "            )\n",
    "            labels = torch.where(\n",
    "                labels == -100,\n",
    "                torch.tensor(tokenizer_target.pad_token_id).to(labels.device),\n",
    "                labels\n",
    "            )\n",
    "            \n",
    "            preds = tokenizer_target.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            refs = tokenizer_target.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            for ref in refs:\n",
    "                sacre_refs.append(str(ref))\n",
    "            \n",
    "            for pred in preds:\n",
    "                sacre_preds.append(str(pred))\n",
    "            \n",
    "            ref_tokens = [ref.strip().split() for ref in refs]\n",
    "            pred_tokens = [pred.strip().split() for pred in preds]\n",
    "            \n",
    "            all_refs.extend([ref] for ref in ref_tokens)\n",
    "            all_preds.extend(pred_tokens)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_eval_loss = eval_loss / len(eval_loader)\n",
    "    bleu1, bleu2, bleu3, bleu4 = quick_bleu_metric(all_refs, all_preds, split=f'{log_what }Validation')\n",
    "    bleu_sacre = sacrebleu.corpus_bleu(sacre_preds, [sacre_refs])\n",
    "    bleu_sacre1, bleu_sacre2, bleu_sacre3, bleu_sacre4 =  bleu_sacre.precisions[0], bleu_sacre.precisions[1], bleu_sacre.precisions[2], bleu_sacre.precisions[3]\n",
    "    # Save best model\n",
    "    # Log metrics\n",
    "    df = pd.DataFrame({\n",
    "                'Reference': [' '.join(ref[0]) for ref in all_refs],\n",
    "                'Prediction': [' '.join(pred) for pred in all_preds]\n",
    "            })\n",
    "    df.to_csv(f'/DATA3/vaibhav/isign/PretrainingISL/helpers/test_csvs/{save_name}.csv', index=False)\n",
    "    \n",
    "    if log_what == \"ISIGN\":\n",
    "        print(f'Sacre Bleu1_Isign :{bleu_sacre1}')\n",
    "        print(f'Sacre Bleu2_Isign :{bleu_sacre2}')\n",
    "        print(f'Sacre Bleu3_Isign :{bleu_sacre3}')\n",
    "        print(f'Sacre Bleu4_Isign :{bleu_sacre4}')\n",
    "\n",
    "        print(f'BLEU1_Isign :{bleu1 * 100}')\n",
    "        print(f'BLEU2_Isign :{bleu2 * 100}')\n",
    "        print(f'BLEU3_Isign :{bleu3 * 100}')\n",
    "        print(f'BLEU4_Isign :{bleu4 * 100}')\n",
    "\n",
    "\n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Resume training\n",
    "    model.train()\n",
    "    feature_projection.train()\n",
    "    return best_val_B4, best_val_loss, best_val_B4_isign, best_val_B1_isign, best_val_loss_isign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1:   0%|          | 0/89 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/el/miniconda3/envs/nmt2/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:629: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/home/el/miniconda3/envs/nmt2/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:649: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/el/miniconda3/envs/nmt2/lib/python3.10/site-packages/transformers/generation/utils.py:1527: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Evaluating Epoch 1: 100%|██████████| 89/89 [01:51<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU ISIGNValidation >>> B1:16.42, B2:7.51, B3:4.62, B4:3.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sacre Bleu1_Isign :21.986722699311773\n",
      "Sacre Bleu2_Isign :4.5882411742165505\n",
      "Sacre Bleu3_Isign :2.3376432289271856\n",
      "Sacre Bleu4_Isign :1.4756890996038832\n",
      "BLEU1_Isign :16.423014999488693\n",
      "BLEU2_Isign :7.508895888785383\n",
      "BLEU3_Isign :4.62098957975436\n",
      "BLEU4_Isign :3.231809095881845\n"
     ]
    }
   ],
   "source": [
    "best_val_B4, best_val_loss, best_val_B4_isign, best_val_B1_isign, best_val_loss_isign = model_eval(\n",
    "                eval2_loader, \"ISIGN\", best_val_B4,best_val_loss, best_val_B4_isign, \n",
    "                best_val_B1_isign, best_val_loss_isign,1,  epoch_steps, epoch_steps, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
