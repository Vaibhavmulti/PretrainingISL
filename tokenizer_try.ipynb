{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_csv = \"/DATA3/vaibhav/isign/PretrainingISL/predictions_new/CISLR_Pretraining_NewMT_GN_RF_MixIsign_NOPT_predictionsISIGNB4.csv\"\n",
    "pred_csv = '/DATA3/vaibhav/isign/PretrainingISL/predictions_new/CISLR_Pretraining_DedupNewMT_GN_RF_MixIsign_PT1_predictionsISIGNB4.csv'\n",
    "gt_labels = '/DATA7/vaibhav/tokenization/val_split_unicode_filtered.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helpers.bleu_cal import quick_bleu_metric\n",
    "pred_df = pd.read_csv(pred_csv)\n",
    "gt_df = pd.read_csv(gt_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU VAl >>> B1:50.00, B2:70.71, B3:79.37, B4:84.09\n"
     ]
    }
   ],
   "source": [
    "bleu1, bleu2, bleu3, bleu4 = quick_bleu_metric(['This is a random sentence'], [['This is a random sentence']], \"VAl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['President Muizzu announced that Maldives has reached an agreement with Europe, USA and Turkey'], ['for the import of the same products and would not import any of these from India.'], ['India had gifted Maldives, radar stations, surveillance aircraft, 2 helicopters, etc.']]\n",
      "['the incident happened before the all india and the first case was filed', '6 ott do share your details', 'it has gar nered a lot of praise from around 4 5 0 0 < PERSON > away from india and august 2 0 2 2']\n"
     ]
    }
   ],
   "source": [
    "ref = gt_df['text'].tolist()\n",
    "ref_list = [[r] for r in ref]\n",
    "predictions = pred_df['Prediction'].tolist()\n",
    "print(ref_list[-3:])\n",
    "print(predictions[-3:])\n",
    "#bleu1, bleu2, bleu3, bleu4 = quick_bleu_metric(ref_list, predictions, split=f'Ising Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 100.00\n",
      "Precisions: [100.0, 100.0, 100.0, 100.0]\n",
      "BP: 1.000\n",
      "Ratio: 1.000\n",
      "Sys-len: 5\n",
      "Ref-len: 5\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "# Initialize BLEU scorer\n",
    "bleu = BLEU()\n",
    "\n",
    "# Convert lists to required format\n",
    "# refs must be a list of lists (multiple references per prediction)\n",
    "# preds must be a list of strings\n",
    "\n",
    "references = ref_list # List of strings\n",
    "\n",
    "# Calculate BLEU\n",
    "bleu_score = bleu.corpus_score(predictions, references)\n",
    "\n",
    "bleu_score = bleu.corpus_score([\"This is a random sentence\"], [['This is a random sentence','random sentence']])\n",
    "\n",
    "# Access scores\n",
    "print(f'BLEU: {bleu_score.score:.2f}')\n",
    "print(f'Precisions: {bleu_score.precisions}')\n",
    "print(f'BP: {bleu_score.bp:.3f}')\n",
    "print(f'Ratio: {bleu_score.ratio:.3f}')\n",
    "print(f'Sys-len: {bleu_score.sys_len}')\n",
    "print(f'Ref-len: {bleu_score.ref_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Ising Validation >>> B1:100.00, B2:86.60, B3:79.37, B4:59.46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0000000000000004,\n",
       " 0.8660254037844388,\n",
       " 0.7937005259841002,\n",
       " 0.5946035575013604)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "def quick_bleu_metric(ref_tokens_list, gen_tokens, split):\n",
    "    # Initialize BLEU with different n-gram orders\n",
    "    bleu1 = BLEU(max_ngram_order=1)\n",
    "    bleu2 = BLEU(max_ngram_order=2)\n",
    "    bleu3 = BLEU(max_ngram_order=3)\n",
    "    bleu4 = BLEU(max_ngram_order=4)\n",
    "    \n",
    "    # Calculate scores\n",
    "    b1 = bleu1.corpus_score(gen_tokens, ref_tokens_list).score\n",
    "    b2 = bleu2.corpus_score(gen_tokens, ref_tokens_list).score\n",
    "    b3 = bleu3.corpus_score(gen_tokens, ref_tokens_list).score\n",
    "    b4 = bleu4.corpus_score(gen_tokens, ref_tokens_list).score\n",
    "    \n",
    "    print(f'BLEU {split} >>> B1:{b1:.2f}, B2:{b2:.2f}, B3:{b3:.2f}, B4:{b4:.2f}')\n",
    "    return b1/100, b2/100, b3/100, b4/100\n",
    "\n",
    "quick_bleu_metric(ref_list, predictions, split=f'Ising Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU score: 100.00000000000004\n",
      "BLEU-1: 100.0000\n",
      "BLEU-2: 100.0000\n",
      "BLEU-3: 100.0000\n",
      "BLEU-4: 100.0000\n",
      "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 12 ref_len = 12)\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "df_gt = pd.DataFrame({'text':['first sentence really working', 'second sentence really working', 'third sentence really working']})\n",
    "df_pred = pd.DataFrame({'Prediction':['first sentence really working', 'second sentence really working', 'third sentence really working']})\n",
    "\n",
    "references = [df_gt['text'].tolist()]  # sacreBLEU expects a list of lists\n",
    "#references = [ [ref] for ref in df_gt['text'].tolist()]\n",
    "predictions = df_pred['Prediction'].tolist()\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(predictions, references)\n",
    "print(f\"sacreBLEU score: {bleu.score}\")\n",
    "\n",
    "print(f\"BLEU-1: {bleu.precisions[0]:.4f}\")\n",
    "print(f\"BLEU-2: {bleu.precisions[1]:.4f}\")\n",
    "print(f\"BLEU-3: {bleu.precisions[2]:.4f}\")\n",
    "print(f\"BLEU-4: {bleu.precisions[3]:.4f}\")\n",
    "\n",
    "print(bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /home/el/miniconda3/envs/nmt2/lib/python3.10/site-packages (from sacrebleu) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/el/miniconda3/envs/nmt2/lib/python3.10/site-packages (from sacrebleu) (2.0.1)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 lxml-5.3.0 portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>video_id</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eabe1f38221a-3</td>\n",
       "      <td>On a school day, you are busy studying, playin...</td>\n",
       "      <td>eabe1f38221a</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eabe1f38221a-5</td>\n",
       "      <td>When you have a holiday from school, what do y...</td>\n",
       "      <td>eabe1f38221a</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eabe1f38221a-7</td>\n",
       "      <td>Fred is a very lazy frog.</td>\n",
       "      <td>eabe1f38221a</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eabe1f38221a-8</td>\n",
       "      <td>who lolls all day upon a log.</td>\n",
       "      <td>eabe1f38221a</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eabe1f38221a-9</td>\n",
       "      <td>He always manages to shirk.</td>\n",
       "      <td>eabe1f38221a</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648</th>\n",
       "      <td>lcBujGRLwE0--34</td>\n",
       "      <td>Now slowly teams are coming back.</td>\n",
       "      <td>lcBujGRLwE0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>lcBujGRLwE0--35</td>\n",
       "      <td>Maldives also announced that it will not impor...</td>\n",
       "      <td>lcBujGRLwE0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>lcBujGRLwE0--36</td>\n",
       "      <td>President Muizzu announced that Maldives has r...</td>\n",
       "      <td>lcBujGRLwE0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>lcBujGRLwE0--37</td>\n",
       "      <td>for the import of the same products and would ...</td>\n",
       "      <td>lcBujGRLwE0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>lcBujGRLwE0--38</td>\n",
       "      <td>India had gifted Maldives, radar stations, sur...</td>\n",
       "      <td>lcBujGRLwE0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5653 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  uid                                               text  \\\n",
       "0      eabe1f38221a-3  On a school day, you are busy studying, playin...   \n",
       "1      eabe1f38221a-5  When you have a holiday from school, what do y...   \n",
       "2      eabe1f38221a-7                          Fred is a very lazy frog.   \n",
       "3      eabe1f38221a-8                      who lolls all day upon a log.   \n",
       "4      eabe1f38221a-9                        He always manages to shirk.   \n",
       "...               ...                                                ...   \n",
       "5648  lcBujGRLwE0--34                  Now slowly teams are coming back.   \n",
       "5649  lcBujGRLwE0--35  Maldives also announced that it will not impor...   \n",
       "5650  lcBujGRLwE0--36  President Muizzu announced that Maldives has r...   \n",
       "5651  lcBujGRLwE0--37  for the import of the same products and would ...   \n",
       "5652  lcBujGRLwE0--38  India had gifted Maldives, radar stations, sur...   \n",
       "\n",
       "          video_id split  \n",
       "0     eabe1f38221a   val  \n",
       "1     eabe1f38221a   val  \n",
       "2     eabe1f38221a   val  \n",
       "3     eabe1f38221a   val  \n",
       "4     eabe1f38221a   val  \n",
       "...            ...   ...  \n",
       "5648   lcBujGRLwE0   val  \n",
       "5649   lcBujGRLwE0   val  \n",
       "5650   lcBujGRLwE0   val  \n",
       "5651   lcBujGRLwE0   val  \n",
       "5652   lcBujGRLwE0   val  \n",
       "\n",
       "[5653 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O n a school day you are busy studying playing...</td>\n",
       "      <td>this course is very easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W hen you have a holiday from school what do y...</td>\n",
       "      <td>which materials are conductors and which are i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>red is a very lazy frog</td>\n",
       "      <td>the next day on 2 6 th june 2 0 2 2 &lt; PERSON &gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who lo ll s all day upon a log</td>\n",
       "      <td>a columbia tie a big turned around it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e always manages to shir k</td>\n",
       "      <td>too got away from a large crowd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648</th>\n",
       "      <td>N ow slowly teams are coming back</td>\n",
       "      <td>this is why they decided to postpone the league</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>al di ves also announced that it will not impo...</td>\n",
       "      <td>and the former president s office at the same ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>P resident u iz z u announced that al di ves h...</td>\n",
       "      <td>the incident happened before the all india and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>for the import of the same products and would ...</td>\n",
       "      <td>6 ott do share your details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>n dia had gifted al di ves rad ar stations sur...</td>\n",
       "      <td>it has gar nered a lot of praise from around 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5653 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference  \\\n",
       "0     O n a school day you are busy studying playing...   \n",
       "1     W hen you have a holiday from school what do y...   \n",
       "2                               red is a very lazy frog   \n",
       "3                        who lo ll s all day upon a log   \n",
       "4                            e always manages to shir k   \n",
       "...                                                 ...   \n",
       "5648                  N ow slowly teams are coming back   \n",
       "5649  al di ves also announced that it will not impo...   \n",
       "5650  P resident u iz z u announced that al di ves h...   \n",
       "5651  for the import of the same products and would ...   \n",
       "5652  n dia had gifted al di ves rad ar stations sur...   \n",
       "\n",
       "                                             Prediction  \n",
       "0                              this course is very easy  \n",
       "1     which materials are conductors and which are i...  \n",
       "2     the next day on 2 6 th june 2 0 2 2 < PERSON >...  \n",
       "3                 a columbia tie a big turned around it  \n",
       "4                       too got away from a large crowd  \n",
       "...                                                 ...  \n",
       "5648    this is why they decided to postpone the league  \n",
       "5649  and the former president s office at the same ...  \n",
       "5650  the incident happened before the all india and...  \n",
       "5651                        6 ott do share your details  \n",
       "5652  it has gar nered a lot of praise from around 4...  \n",
       "\n",
       "[5653 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pred_df['Prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/el/miniconda3/envs/nmt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "50258\n",
      "50259\n",
      "50260\n",
      "50261\n",
      "[50262, 50263]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer  # Add this import\n",
    "tokenizer_target = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer_target.add_special_tokens({\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    'additional_special_tokens': ['<PERSON>', '<UNKNOWN>']\n",
    "})\n",
    "\n",
    "#Print the tokenid with respect to these tokens\n",
    "print(tokenizer_target.bos_token_id)\n",
    "print(tokenizer_target.eos_token_id)\n",
    "print(tokenizer_target.unk_token_id)\n",
    "print(tokenizer_target.pad_token_id)\n",
    "print(tokenizer_target.mask_token_id)\n",
    "print(tokenizer_target.additional_special_tokens_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,    11,   616,   220, 50262,   318,   220, 50263,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Hello, my  <PERSON>  is  <UNKNOWN> !\n"
     ]
    }
   ],
   "source": [
    "# Encode a text\n",
    "text = \"Hello, my <PERSON> is <UNKNOWN>!\"\n",
    "encoded_input = tokenizer_target(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "\n",
    "# Decode the encoded tokens back to a text\n",
    "decoded_output = tokenizer_target.decode(encoded_input['input_ids'][0])\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/el/miniconda3/envs/nmt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "project_name = \"CISLR_Pretraining\"\n",
    "sub_project_name = \"NewMT_GN_RF_MixIsign_PT1\"\n",
    "run_name = \"NewMT_GN_RF_MixIsign_PT1\"\n",
    "\n",
    "# Gausian Noise , Random frame sampling , Isign mixed with CISLR linearly\n",
    "\n",
    "randomize_word_order = False\n",
    "steps_for_100percentIsign = 60000\n",
    "import os\n",
    "import pandas as pd\n",
    "# # Set the visible GPU devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('/DATA3/vaibhav/isign/PretrainingISL/train_MT_dedup.csv')\n",
    "eval_df = pd.read_csv('/DATA3/vaibhav/isign/PretrainingISL/val_MT_dedup.csv')\n",
    "test_df = pd.read_csv('/DATA3/vaibhav/isign/PretrainingISL/test_MT_dedup.csv')\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import gc\n",
    "import collections\n",
    "import math\n",
    "import ast\n",
    "import collections\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig, BertModel,\n",
    "    GPT2Config, GPT2LMHeadModel, \n",
    "    EncoderDecoderModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    get_constant_schedule_with_warmup\n",
    ")\n",
    "from transformers import GPT2Tokenizer  # Add this import\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from helpers.bleu_cal import quick_bleu_metric\n",
    "from helpers.dataloaders import FeatureVectorDataset, FeatureVectorDataset_Isign\n",
    "from pose_format import Pose\n",
    "from pose_format.pose_visualizer import PoseVisualizer\n",
    "from itertools import cycle\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed()\n",
    "\n",
    "def get_threshold(current_step, total_steps):\n",
    "    if total_steps == 0:\n",
    "        return 1.1\n",
    "    else:\n",
    "        return min(current_step / total_steps, 0.9)\n",
    "\n",
    "#Hyperparameters here now \n",
    "learning_rate = 3e-4 \n",
    "num_encoder_layers = 4 #6\n",
    "num_decoder_layers = 4 #6\n",
    "encoder_hidden_size = 512 #512\n",
    "decoder_hidden_size = 512 #512\n",
    "num_attention_heads = 8\n",
    "dropout = 0.1\n",
    "MAX_FRAMES = 300  # Max video frames.\n",
    "max_position_embeddings_encoder = MAX_FRAMES\n",
    "num_beams = 3\n",
    "#label_smoothing = 0.1 not used yet\n",
    "warmup_steps_ratio = 0.1\n",
    "batch_size = 16 #64 #256\n",
    "#gradient_accumulation_steps = 1 not used yet\n",
    "lr_scheduler_type = 'warmup_linear_constant_afterwards'\n",
    "num_epochs = 1\n",
    "max_length_decoder = 128\n",
    "vocab_size_decoder = 15000\n",
    "num_keypoints = 152 # We have cherrypicked these\n",
    "WEIGTH_DECAY = 0.01\n",
    "\n",
    "POSE_DIR = \"/DATA7/vaibhav/tokenization/CISLR/CISLR_v1.5-a_videos_poses/\"\n",
    "POSE_DIR_ISIGN = \"/DATA7/vaibhav/isign/Data/iSign-poses_v1.1/\"\n",
    "STEP_FRAMES = None  # Random sampling of frames.\n",
    "ADD_NOISE_ISIGN = False\n",
    "ADD_NOISE_CISLR = True\n",
    "\n",
    "\n",
    "hyperparameters = {'learning_rate': learning_rate, \n",
    "                     'num_encoder_layers': num_encoder_layers,\n",
    "                        'num_decoder_layers': num_decoder_layers,\n",
    "                        'encoder_hidden_size': encoder_hidden_size,\n",
    "                        'decoder_hidden_size': decoder_hidden_size,\n",
    "                        'num_attention_heads': num_attention_heads,\n",
    "                        'dropout': dropout,\n",
    "                        'max_position_embeddings_encoder': max_position_embeddings_encoder,\n",
    "                        'num_beams': num_beams,\n",
    "                        'warmup_steps_ratio': warmup_steps_ratio,\n",
    "                        'batch_size': batch_size,\n",
    "                        'lr_scheduler_type': lr_scheduler_type,\n",
    "                        'num_epochs': num_epochs,\n",
    "                        'max_length_decoder': max_length_decoder,\n",
    "                        'Max_frames_videos': MAX_FRAMES,\n",
    "                        'vocab_size_decoder': vocab_size_decoder,\n",
    "                        'num_keypoints': num_keypoints,\n",
    "                        'POSE_DIR': POSE_DIR,\n",
    "                        'POSE_DIR_ISIGN': POSE_DIR_ISIGN,\n",
    "                        'randomize_word_order': randomize_word_order\n",
    "                        ,'sub_project_name': sub_project_name,\n",
    "                        'steps_for_100percentIsign': steps_for_100percentIsign,\n",
    "                        'ADD_NOISE_ISIGN': ADD_NOISE_ISIGN,\n",
    "                        'ADD_NOISE_CISLR': ADD_NOISE_CISLR,\n",
    "                        'Step_frames_sampling': STEP_FRAMES}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#wandb.init(project=project_name, name=run_name, config = hyperparameters)\n",
    "\n",
    "#wandb.init(project=project_name, config = hyperparameters, id=\"c43wvsr1\", resume=\"must\")\n",
    "\n",
    "#wandb.init(project=project_name, config = hyperparameters, id=\"7ike4lk8\", resume=\"must\")\n",
    "\n",
    "\n",
    "eval_df2 = pd.read_csv('/DATA7/vaibhav/tokenization/val_split_unicode_filtered.csv')\n",
    "#'/DATACSEShare/sanjeet/Dataset/Sign_lanuguage_data_set/isign/Final_Processed_raw_sentences_isign.csv'\n",
    "#train_df2 = pd.read_csv('/DATA7/vaibhav/tokenization/train_split_unicode_filtered.csv')\n",
    "train_df2 = pd.read_csv(\"/DATA3/vaibhav/isign/PretrainingISL/isign_new.csv\")\n",
    "# Step 2: Train Tokenizers\n",
    "# Combine source and target sequences for a joint tokenizer\n",
    "#all_sequences = train_df['SENTENCE_UNICIDE'].tolist() + train_df['text'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "all_sequences_target = train_df['text'].tolist() + train_df2['text'].tolist()\n",
    "#all_sequences_target = train_df['text'].values.tolist() + train_df2['text'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_target = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_target = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer_target.add_special_tokens({\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting video UIDs and labels...\n",
      "Appending <s> and </s> to labels...\n",
      "Tokenizing labels...\n",
      "Creating datasets...\n",
      "Creating DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "print('Extracting video UIDs and labels...')\n",
    "train_video_uids = train_df['uid_list'].apply(ast.literal_eval).tolist()\n",
    "eval_video_uids = eval_df['uid_list'].apply(ast.literal_eval).tolist()\n",
    "test_video_uids = test_df['uid_list'].apply(ast.literal_eval).tolist()\n",
    "\n",
    "train2_video_uids = train_df2['uid'].tolist()\n",
    "eval2_video_uids = eval_df2['uid'].tolist()\n",
    "\n",
    "print('Appending <s> and </s> to labels...')\n",
    "train_labels = [f'<s>{text}</s>' for text in train_df['text'].tolist()]\n",
    "eval_labels = [f'<s>{text}</s>' for text in eval_df['text'].tolist()]\n",
    "test_labels = [f'<s>{text}</s>' for text in test_df['text'].tolist()]\n",
    "\n",
    "train2_labels = [f'<s>{text}</s>' for text in train_df2['text'].tolist()]\n",
    "eval2_labels = [f'<s>{text}</s>' for text in eval_df2['text'].tolist()]\n",
    "\n",
    "\n",
    "def tokenize_in_batches(texts, tokenizer, max_length, batch_size=1000):\n",
    "    all_tokens = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        tokens = tokenizer(\n",
    "            batch, \n",
    "            max_length=max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        all_tokens.extend(tokens)\n",
    "    return all_tokens\n",
    "\n",
    "\n",
    "# Tokenize labels\n",
    "print('Tokenizing labels...')\n",
    "train_labels = tokenize_in_batches(train_labels, tokenizer_target, max_length_decoder)\n",
    "#train_labels = tokenizer_target(train_labels, max_length=max_length_decoder, padding=\"max_length\", truncation=True)['input_ids']\n",
    "eval_labels = tokenizer_target(eval_labels, max_length=max_length_decoder, padding=\"max_length\", truncation=True)['input_ids']\n",
    "test_labels = tokenizer_target(test_labels, max_length=max_length_decoder, padding=\"max_length\", truncation=True)['input_ids']\n",
    "\n",
    "train2_labels = tokenizer_target(train2_labels, max_length=max_length_decoder, padding=\"max_length\", truncation=True)['input_ids']\n",
    "eval2_labels = tokenizer_target(eval2_labels, max_length=max_length_decoder, padding=\"max_length\", truncation=True)['input_ids']\n",
    "\n",
    "# Create datasets\n",
    "print('Creating datasets...')\n",
    "\n",
    "\n",
    "train_dataset = FeatureVectorDataset(train_video_uids,tokenizer_target,\n",
    "                                      randomize_word_order, MAX_FRAMES, POSE_DIR,\n",
    "                                      train_labels, step_frames=STEP_FRAMES, add_noise = ADD_NOISE_CISLR)\n",
    "test_dataset = FeatureVectorDataset(test_video_uids,tokenizer_target, \n",
    "                                    randomize_word_order, MAX_FRAMES, POSE_DIR,\n",
    "                                    test_labels,step_frames=STEP_FRAMES, add_noise = ADD_NOISE_CISLR)\n",
    "eval_dataset = FeatureVectorDataset(eval_video_uids,tokenizer_target, \n",
    "                                    randomize_word_order,MAX_FRAMES, POSE_DIR,\n",
    "                                    eval_labels,step_frames=STEP_FRAMES, add_noise = ADD_NOISE_CISLR)\n",
    "\n",
    "train2_dataset = FeatureVectorDataset_Isign(train2_video_uids, tokenizer_target,\n",
    "                                            MAX_FRAMES, POSE_DIR_ISIGN, train2_labels ,\n",
    "                                           step_frames=STEP_FRAMES, add_noise = ADD_NOISE_ISIGN)\n",
    "\n",
    "eval2_dataset = FeatureVectorDataset_Isign(eval2_video_uids, tokenizer_target, \n",
    "                                        MAX_FRAMES, POSE_DIR_ISIGN, eval2_labels, \n",
    "                                        step_frames=STEP_FRAMES, add_noise = ADD_NOISE_ISIGN)\n",
    "\n",
    "# tp_tensor = torch.tensor([    0,   146,   124,  2562,   450,   144,  1785,   133,  8466,     2], dtype=torch.long)\n",
    "# print(tokenizer_target.convert_ids_to_tokens(tp_tensor))\n",
    "# Create DataLoaders\n",
    "print('Creating DataLoaders...')\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=2,  # Reduced workers\n",
    "#     pin_memory=True,\n",
    "#     prefetch_factor=2,  # Add prefetch\n",
    "#     persistent_workers=True  # Keep workers alive\n",
    "# )\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True, prefetch_factor=2)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, num_workers=32, pin_memory=True, prefetch_factor=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=32, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "isign_loader = DataLoader(train2_dataset, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True, prefetch_factor=2)\n",
    "eval2_loader = DataLoader(eval2_dataset, batch_size=batch_size, num_workers=32, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "isign_loader_cycle = cycle(isign_loader)  # To cycle through ISIGN when exhausted\n",
    "cislr_loader_cycle = cycle(train_loader)  # To cycle through CISLR when exhausted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50257,  2946,   290,  ...,  -100,  -100,  -100],\n",
      "        [50257,    72,   716,  ...,  -100,  -100,  -100],\n",
      "        [50257,   896,  6194,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [50257,   568,   340,  ...,  -100,  -100,  -100],\n",
      "        [50257, 10919,   546,  ...,  -100,  -100,  -100],\n",
      "        [50257,  8340,   262,  ...,  -100,  -100,  -100]], device='cuda:0')\n",
      "['hold and release', 'i am happy that the film is doing well', 'its symbol is the gun', 'reach out to the cell phone', 'he has pain in the back and neck', '1 9 4 1 today jeep', 'all sea parts in the ship', 'no foreign government organization or individual can interfere', 'do you remember where you were on june 1 2 th 2 0 1 6', 'but good god man', 'stay mobile', 'i guess that is all i can ask', 'so where do you stand', 'so it can be very small to big', 'what about its taste', 'watch the video below to know more']\n",
      "[['hold', 'and', 'release'], ['i', 'am', 'happy', 'that', 'the', 'film', 'is', 'doing', 'well'], ['its', 'symbol', 'is', 'the', 'gun'], ['reach', 'out', 'to', 'the', 'cell', 'phone'], ['he', 'has', 'pain', 'in', 'the', 'back', 'and', 'neck'], ['1', '9', '4', '1', 'today', 'jeep'], ['all', 'sea', 'parts', 'in', 'the', 'ship'], ['no', 'foreign', 'government', 'organization', 'or', 'individual', 'can', 'interfere'], ['do', 'you', 'remember', 'where', 'you', 'were', 'on', 'june', '1', '2', 'th', '2', '0', '1', '6'], ['but', 'good', 'god', 'man'], ['stay', 'mobile'], ['i', 'guess', 'that', 'is', 'all', 'i', 'can', 'ask'], ['so', 'where', 'do', 'you', 'stand'], ['so', 'it', 'can', 'be', 'very', 'small', 'to', 'big'], ['what', 'about', 'its', 'taste'], ['watch', 'the', 'video', 'below', 'to', 'know', 'more']]\n"
     ]
    }
   ],
   "source": [
    "all_refs = []\n",
    "all_preds = []\n",
    "\n",
    "for eval_batch in eval_loader:\n",
    "    input_ids = eval_batch['input_ids'].to(device)\n",
    "    attention_mask = eval_batch['attention_mask'].to(device)\n",
    "    labels = eval_batch['labels'].to(device)\n",
    "\n",
    "    print(labels)\n",
    "    labels = torch.where(\n",
    "                labels == -100,\n",
    "                torch.tensor(tokenizer_target.pad_token_id).to(labels.device),\n",
    "                labels\n",
    "            )\n",
    "    \n",
    "    refs = tokenizer_target.batch_decode(labels, skip_special_tokens=True)\n",
    "    print(refs)\n",
    "    #ref_tokens = [ref.strip().split() for ref in refs]\n",
    "    print(ref_tokens)\n",
    "    \n",
    "    all_refs.extend([ref] for ref in refs)\n",
    "    #all_preds.extend(pred_tokens)\n",
    "\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['hold', 'and', 'release']], [['i', 'am', 'happy', 'that', 'the', 'film', 'is', 'doing', 'well']], [['its', 'symbol', 'is', 'the', 'gun']], [['reach', 'out', 'to', 'the', 'cell', 'phone']], [['he', 'has', 'pain', 'in', 'the', 'back', 'and', 'neck']], [['1', '9', '4', '1', 'today', 'jeep']], [['all', 'sea', 'parts', 'in', 'the', 'ship']], [['no', 'foreign', 'government', 'organization', 'or', 'individual', 'can', 'interfere']], [['do', 'you', 'remember', 'where', 'you', 'were', 'on', 'june', '1', '2', 'th', '2', '0', '1', '6']], [['but', 'good', 'god', 'man']], [['stay', 'mobile']], [['i', 'guess', 'that', 'is', 'all', 'i', 'can', 'ask']], [['so', 'where', 'do', 'you', 'stand']], [['so', 'it', 'can', 'be', 'very', 'small', 'to', 'big']], [['what', 'about', 'its', 'taste']], [['watch', 'the', 'video', 'below', 'to', 'know', 'more']]]\n"
     ]
    }
   ],
   "source": [
    "print(all_refs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
